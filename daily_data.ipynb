{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "df = pd.read_csv(\"consumption.csv\")\n",
    "weather_avg = pd.read_csv('weather-avg.csv')\n",
    "weather_min = pd.read_csv('weather-min.csv')\n",
    "weather_max = pd.read_csv('weather-max.csv')\n",
    "info = pd.read_csv('addinfo.csv')\n",
    "weather_avg = weather_avg.set_index(\"meter_id\")\n",
    "weather_min = weather_min.set_index(\"meter_id\")\n",
    "weather_max = weather_max.set_index(\"meter_id\")\n",
    "info = info.set_index(\"meter_id\")\n",
    "weather_avg_sorted = weather_avg.reindex(df.iloc[:,0])\n",
    "weather_min_sorted = weather_min.reindex(df.iloc[:,0])\n",
    "weather_max_sorted = weather_max.reindex(df.iloc[:,0])\n",
    "info_sorted = info.reindex(df.iloc[:,0])\n",
    "brinfo=info_sorted['num_bedrooms']\n",
    "values = {'num_bedrooms' : brinfo.mean()}\n",
    "info_filled_br = info_sorted.fillna(value = values)\n",
    "brinfo_filled = info_filled_br['num_bedrooms']\n",
    "#functions\n",
    "def get_monthi(n):\n",
    "    begin=48*31*(n-1)+1\n",
    "    end=48*31*n\n",
    "    if n==1:\n",
    "        begin=1\n",
    "    if n>1:\n",
    "        end-=3*48\n",
    "    if n>2:\n",
    "        begin-=3*48\n",
    "    if n>3:\n",
    "        end-=48\n",
    "    if n>4:\n",
    "        begin-=48\n",
    "    if n>5:\n",
    "        end-=48\n",
    "    if n>6:\n",
    "        begin-=48\n",
    "    if n>8:\n",
    "        end-=48\n",
    "    if n>9:\n",
    "        begin-=48\n",
    "    if n>10:\n",
    "        end-=48\n",
    "    if n>11:\n",
    "        begin-=48\n",
    "    return begin,end\n",
    "\n",
    "def get_mean_temp(row,month):\n",
    "    \"\"\"\n",
    "    row: is the row (meter_id) we would like to get the average temperature for.\n",
    "    month: which month (columns) we would get the average temperature for.\n",
    "    returns: the average temperature for a specific meter_id for a specific month.\n",
    "    \"\"\"\n",
    "    if month==1:\n",
    "        return row.loc[:,\"2017-01-01 00:00:00\":\"2017-01-31 00:00:00\"].mean(1)\n",
    "    elif month==2:\n",
    "        return row.loc[:,\"2017-02-01 00:00:00\":\"2017-02-28 00:00:00\"].mean(1)\n",
    "    elif month==3:\n",
    "        return row.loc[:,\"2017-03-01 00:00:00\":\"2017-03-31 00:00:00\"].mean(1)\n",
    "    elif month==4:\n",
    "        return row.loc[:,\"2017-04-01 00:00:00\":\"2017-04-30 00:00:00\"].mean(1)\n",
    "    elif month==5:\n",
    "        return row.loc[:,\"2017-05-01 00:00:00\":\"2017-05-31 00:00:00\"].mean(1)\n",
    "    elif month==6:\n",
    "        return row.loc[:,\"2017-06-01 00:00:00\":\"2017-06-30 00:00:00\"].mean(1)\n",
    "    elif month==7:\n",
    "        return row.loc[:,\"2017-07-01 00:00:00\":\"2017-07-31 00:00:00\"].mean(1)\n",
    "    elif month==8:\n",
    "        return row.loc[:,\"2017-08-01 00:00:00\":\"2017-08-31 00:00:00\"].mean(1)\n",
    "    elif month==9:\n",
    "        return row.loc[:,\"2017-09-01 00:00:00\":\"2017-09-30 00:00:00\"].mean(1)\n",
    "    elif month==10:\n",
    "        return row.loc[:,\"2017-10-01 00:00:00\":\"2017-10-31 00:00:00\"].mean(1)\n",
    "    elif month==11:\n",
    "        return row.loc[:,\"2017-11-01 00:00:00\":\"2017-11-30 00:00:00\"].mean(1)\n",
    "    elif month==12:\n",
    "        return row.loc[:,\"2017-12-01 00:00:00\":\"2017-12-31 00:00:00\"].mean(1)\n",
    "    else:\n",
    "        print(\"Error: this is not a valid input for month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_t=1200\n",
    "En_con=[]\n",
    "l_En_con=[]\n",
    "\n",
    "for i in range(df.shape[0]): # loop over all users\n",
    "    meter=df.iloc[i] # = row number i  \n",
    "    \n",
    "    for m in range (2,13): # loop over all months\n",
    "        #load current month\n",
    "        bi, ei =get_monthi(m)               #bi (begin index) ei(end index)\n",
    "        month=meter[bi:ei]  #data for the month m for the row user (row) i\n",
    "        #Load last month\n",
    "        lbi,lei=get_monthi(m-1)             #lbi (last begin index) lei (last end index)\n",
    "        last_month=meter[lbi:lei] \n",
    "        \n",
    "        # Count the amount of NaN's in current and last month      \n",
    "        n_NaN=month.isnull().sum() \n",
    "        l_n_NaN=last_month.isnull().sum()\n",
    "        \n",
    "        # Check if there are any NaN's in current and last month\n",
    "        if n_NaN<NaN_t and l_n_NaN==0:\n",
    "            En_con.append(month.mean())\n",
    "            # number of days in the last month\n",
    "            n_day=np.int((lei-lbi+1)/48)\n",
    "            # daily consumption last month\n",
    "            # all months will be considered 31 days since the model needs a constant input size\n",
    "            # months with less then 31 days will get the averagen comsumption assigned to the remaining days\n",
    "            daily_con=np.zeros(31)\n",
    "            if n_day < 31:\n",
    "                #fill in daily data with real data\n",
    "                for j in range(n_day):\n",
    "                    daily_con[j]=last_month[j*48:(j+1)*48].mean()\n",
    "                #fill in daily data with the mean\n",
    "                mean_month=last_month.mean()\n",
    "                for j in range(n_day,31):\n",
    "                    daily_con[j]=mean_month\n",
    "            else:\n",
    "                for j in range(31):\n",
    "                    daily_con[j]=last_month[j*48:(j+1)*48].mean()\n",
    "            if np.isnan(np.sum(daily_con)):\n",
    "                print(\"m=%i ,index=%i\" %(m-1,i))\n",
    "                print(last_month)\n",
    "                print(daily_con)\n",
    "            l_En_con.append(daily_con)  \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of days in a month\n",
    "b,e=get_monthi(12)\n",
    "(e-b+1)/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "#some tests\n",
    "meter=df.iloc[0] # = row number i  \n",
    "l_begin_index,l_end_index=get_monthi(12)\n",
    "last_month=meter[l_begin_index:l_end_index+1] #load last \n",
    "meter[l_begin_index]\n",
    "nul=np.zeros(31)\n",
    "nul[2]=3\n",
    "i=30\n",
    "#print(last_month[i*48:(i+1)*48])\n",
    "for i in range(30,31): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3588, 31)\n"
     ]
    }
   ],
   "source": [
    "#print(l_En_con)\n",
    "npl_En_con=np.array(l_En_con)\n",
    "npEn_con = np.zeros(len(En_con))\n",
    "npEn_con=np.array(En_con).ravel()\n",
    "print(npl_En_con.shape) #only 3588 data points/months remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2588, 31)\n"
     ]
    }
   ],
   "source": [
    "X_train = npl_En_con[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 0.01651568  0.00458183  0.02064142  0.06776875 -0.02990141 -0.02470747\n",
      "   0.04329187  0.03475493  0.02203448  0.03075617  0.00149132  0.01580567\n",
      "  -0.01895977  0.0016443  -0.00715614  0.01430695 -0.00110777  0.00722975\n",
      "   0.03515705  0.06505902  0.02848955  0.09322874  0.00034209  0.06460545\n",
      "   0.00199705  0.03252616  0.03168476  0.01965189  0.01201485  0.09522453\n",
      "   0.19844906]]\n",
      "Mean squared error: 0.00165948\n",
      "Coefficient of determination: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last coefficient seems to be the most important. This is a result of adding the imaginary extra days on 6 of 12 months. I'm not sure if the other coefficients actually contributed or that they're just noise. There is a significant improvement to the r^2 value but this might have something to do with the seleceted data and test set aswell. To check this we'll have to run an extra check we're we only use the mean as input over this data set just like in \"linear_model_past_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now construct a array with all the means of the previously used months. It would be easier to loop over the previously generater l_En_con array but this would result in a small error because of the imputed data at the end of some months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_En_con_mean=[]\n",
    "for i in range(df.shape[0]): # loop over all users\n",
    "    meter=df.iloc[i] # = row number i  \n",
    "    \n",
    "    for m in range (2,13): # loop over all months\n",
    "        #load current month\n",
    "        bi, ei =get_monthi(m)               #bi (begin index) ei(end index)\n",
    "        month=meter[bi:ei]  #data for the month m for the row user (row) i\n",
    "        #Load last month\n",
    "        lbi,lei=get_monthi(m-1)             #lbi (last begin index) lei (last end index)\n",
    "        last_month=meter[lbi:lei] \n",
    "        \n",
    "        # Count the amount of NaN's in current and last month      \n",
    "        n_NaN=month.isnull().sum() \n",
    "        l_n_NaN=last_month.isnull().sum()\n",
    "        \n",
    "        # Check if there are any NaN's in current and last month\n",
    "        if n_NaN<NaN_t and l_n_NaN==0:\n",
    "            l_En_con_mean.append(last_month.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3588, 1)\n",
      "[[0.56780959]\n",
      " [0.13408617]\n",
      " [0.11767755]\n",
      " ...\n",
      " [0.49908637]\n",
      " [0.24658652]\n",
      " [0.13918485]]\n"
     ]
    }
   ],
   "source": [
    "npl_En_con_mean = np.zeros(len(l_En_con_mean))\n",
    "npl_En_con_mean=np.array(l_En_con_mean).ravel()\n",
    "npl_En_con_mean=npl_En_con_mean.reshape(-1,1)\n",
    "print(npl_En_con_mean.shape)\n",
    "print(npl_En_con_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a new linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[0.85518305]]\n",
      "Mean squared error: 0.00183649\n",
      "Coefficient of determination: 0.8846\n"
     ]
    }
   ],
   "source": [
    "X_train = npl_En_con_mean[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_mean[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like most of the improments were due to the smaller data set and not the inclusiong of the daily information. It could also be that our linear model can not take advantages of the extra information and just makes a sligtly different or more randomised mean of our months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check what the effect is of not imputing any data an just trowing away the end of the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_t=1200\n",
    "En_con=[]\n",
    "l_En_con_short=[]\n",
    "\n",
    "for i in range(df.shape[0]): # loop over all users\n",
    "    meter=df.iloc[i] # = row number i  \n",
    "    \n",
    "    for m in range (2,13): # loop over all months\n",
    "        #load current month\n",
    "        bi, ei =get_monthi(m)               #bi (begin index) ei(end index)\n",
    "        month=meter[bi:ei]  #data for the month m for the row user (row) i\n",
    "        #Load last month\n",
    "        lbi,lei=get_monthi(m-1)             #lbi (last begin index) lei (last end index)\n",
    "        last_month=meter[lbi:lei] \n",
    "        \n",
    "        # Count the amount of NaN's in current and last month      \n",
    "        n_NaN=month.isnull().sum() \n",
    "        l_n_NaN=last_month.isnull().sum()\n",
    "        \n",
    "        # Check if there are any NaN's in current and last month\n",
    "        if n_NaN<NaN_t and l_n_NaN==0:\n",
    "            En_con.append(month.mean())\n",
    "            # number of days in the last month\n",
    "            n_day=np.int((lei-lbi+1)/48)\n",
    "            # daily consumption last month\n",
    "            # all months will be considered 31 days since the model needs a constant input size\n",
    "            # months with less then 31 days will get the averagen comsumption assigned to the remaining days\n",
    "            daily_con=np.zeros(28)\n",
    "            for j in range(28):\n",
    "                daily_con[j]=last_month[j*48:(j+1)*48].mean()\n",
    "            l_En_con_short.append(daily_con)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3588, 28)\n"
     ]
    }
   ],
   "source": [
    "npl_En_con_short=np.array(l_En_con_short)\n",
    "npEn_con = np.zeros(len(En_con))\n",
    "npEn_con=np.array(En_con).ravel()\n",
    "print(npl_En_con_short.shape) #only 3588 data points/months remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the linear model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 0.00552543  0.00902142  0.01006986  0.08388736 -0.03056483 -0.03082037\n",
      "   0.04670765  0.0474355   0.05305962  0.04668855  0.01867475  0.01445524\n",
      "  -0.00073413  0.00802702  0.01864846  0.03753931  0.00350489  0.00993871\n",
      "   0.04227988  0.07914947  0.02177436  0.09020664  0.0189688   0.0740619\n",
      "   0.01166754  0.04242231  0.07178077  0.05994506]]\n",
      "Mean squared error: 0.00172722\n",
      "Coefficient of determination: 0.8915\n"
     ]
    }
   ],
   "source": [
    "X_train = npl_En_con_short[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_short[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still better even without the added data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some polynomail tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "months with imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial regression:\n",
    "X_train = npl_En_con[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "def polfit(X_train,X_test,y_train,y_test,deg,printcoef=False):\n",
    "    # Fitting Polynomial Regression to the dataset\n",
    "    # source: https://towardsdatascience.com/machine-learning-polynomial-regression-with-python-5328e4e8a386\n",
    "    # Create polynomial regression object\n",
    "    polynom_degree = deg\n",
    "\n",
    "    poly_reg = PolynomialFeatures(degree = polynom_degree)\n",
    "\n",
    "    X_poly = poly_reg.fit_transform(X_train) # Fit to data, then transform it to \n",
    "    #a new feature matrix consisting ofall polynomial combinations of the features\n",
    "\n",
    "    pol_reg = LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    pol_reg.fit(X_poly, y_train)\n",
    "\n",
    "    # polyfitting the training data:\n",
    "    poly_vals = pol_reg.predict(poly_reg.fit_transform(X_train))\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_poly = pol_reg.predict(poly_reg.fit_transform(X_test))\n",
    "\n",
    "    # The coefficients\n",
    "    if printcoef:\n",
    "        print('Coefficients: \\n', pol_reg.coef_)\n",
    "        \n",
    "    # The mean squared error\n",
    "    print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred_poly))\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    print('Coefficient of determination: %.8f'\n",
    "      % r2_score(y_test, y_pred_poly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.00369047\n",
      "Coefficient of determination: 0.76817884\n",
      "Mean squared error: 0.00183721\n",
      "Coefficient of determination: 0.88459352\n",
      "Mean squared error: 0.00306837\n",
      "Coefficient of determination: 0.80725664\n"
     ]
    }
   ],
   "source": [
    "#imputed data\n",
    "X_train = npl_En_con[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "polfit(X_train,X_test,y_train,y_test,2)\n",
    "\n",
    "#mean vals\n",
    "X_train = npl_En_con_mean[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_mean[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "polfit(X_train,X_test,y_train,y_test,2)\n",
    "\n",
    "#drop ens of months\n",
    "\n",
    "X_train = npl_En_con_short[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_short[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "polfit(X_train,X_test,y_train,y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a neural netword can use this extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed data\n",
    "X_train = npl_En_con[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Train on 2588 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "2588/2588 [==============================] - 1s 331us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 2/50\n",
      "2588/2588 [==============================] - 0s 88us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 3/50\n",
      "2588/2588 [==============================] - 0s 101us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 4/50\n",
      "2588/2588 [==============================] - 0s 90us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 5/50\n",
      "2588/2588 [==============================] - 0s 109us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 6/50\n",
      "2588/2588 [==============================] - 0s 94us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 7/50\n",
      "2588/2588 [==============================] - 0s 86us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 8/50\n",
      "2588/2588 [==============================] - 0s 103us/sample - loss: 9.8828e-04 - mean_squared_error: 9.8828e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 9/50\n",
      "2588/2588 [==============================] - 0s 100us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 10/50\n",
      "2588/2588 [==============================] - 0s 109us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 11/50\n",
      "2588/2588 [==============================] - 0s 100us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 12/50\n",
      "2588/2588 [==============================] - 0s 92us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 13/50\n",
      "2588/2588 [==============================] - 0s 108us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 14/50\n",
      "2588/2588 [==============================] - 0s 104us/sample - loss: 9.9984e-04 - mean_squared_error: 9.9984e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 15/50\n",
      "2588/2588 [==============================] - 0s 100us/sample - loss: 9.7643e-04 - mean_squared_error: 9.7643e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 16/50\n",
      "2588/2588 [==============================] - 0s 96us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 17/50\n",
      "2588/2588 [==============================] - 0s 106us/sample - loss: 9.5069e-04 - mean_squared_error: 9.5069e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 18/50\n",
      "2588/2588 [==============================] - 0s 95us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 19/50\n",
      "2588/2588 [==============================] - 0s 98us/sample - loss: 9.6666e-04 - mean_squared_error: 9.6666e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 20/50\n",
      "2588/2588 [==============================] - 0s 108us/sample - loss: 8.6197e-04 - mean_squared_error: 8.6197e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 21/50\n",
      "2588/2588 [==============================] - 0s 101us/sample - loss: 9.5149e-04 - mean_squared_error: 9.5149e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 22/50\n",
      "2588/2588 [==============================] - 0s 110us/sample - loss: 8.5295e-04 - mean_squared_error: 8.5295e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 23/50\n",
      "2588/2588 [==============================] - 0s 96us/sample - loss: 9.5616e-04 - mean_squared_error: 9.5616e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 24/50\n",
      "2588/2588 [==============================] - 0s 117us/sample - loss: 9.2558e-04 - mean_squared_error: 9.2558e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 25/50\n",
      "2588/2588 [==============================] - 0s 98us/sample - loss: 9.9368e-04 - mean_squared_error: 9.9368e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 26/50\n",
      "2588/2588 [==============================] - 0s 97us/sample - loss: 8.6408e-04 - mean_squared_error: 8.6408e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 27/50\n",
      "2588/2588 [==============================] - 0s 116us/sample - loss: 8.8792e-04 - mean_squared_error: 8.8792e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 28/50\n",
      "2588/2588 [==============================] - 0s 130us/sample - loss: 8.3797e-04 - mean_squared_error: 8.3797e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 29/50\n",
      "2588/2588 [==============================] - 0s 114us/sample - loss: 8.3829e-04 - mean_squared_error: 8.3829e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 30/50\n",
      "2588/2588 [==============================] - 0s 104us/sample - loss: 9.3440e-04 - mean_squared_error: 9.3440e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 31/50\n",
      "2588/2588 [==============================] - 0s 125us/sample - loss: 8.4839e-04 - mean_squared_error: 8.4839e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 32/50\n",
      "2588/2588 [==============================] - 0s 94us/sample - loss: 9.8915e-04 - mean_squared_error: 9.8915e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 33/50\n",
      "2588/2588 [==============================] - 0s 148us/sample - loss: 9.4213e-04 - mean_squared_error: 9.4213e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 34/50\n",
      "2588/2588 [==============================] - 0s 134us/sample - loss: 8.6407e-04 - mean_squared_error: 8.6407e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 35/50\n",
      "2588/2588 [==============================] - 0s 117us/sample - loss: 8.3780e-04 - mean_squared_error: 8.3780e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 36/50\n",
      "2588/2588 [==============================] - 0s 89us/sample - loss: 8.4750e-04 - mean_squared_error: 8.4750e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 37/50\n",
      "2588/2588 [==============================] - 0s 108us/sample - loss: 8.7113e-04 - mean_squared_error: 8.7113e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 38/50\n",
      "2588/2588 [==============================] - 0s 119us/sample - loss: 9.6289e-04 - mean_squared_error: 9.6289e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 39/50\n",
      "2588/2588 [==============================] - 0s 121us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 40/50\n",
      "2588/2588 [==============================] - 0s 112us/sample - loss: 8.8791e-04 - mean_squared_error: 8.8791e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 41/50\n",
      "2588/2588 [==============================] - 0s 124us/sample - loss: 7.7152e-04 - mean_squared_error: 7.7152e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 42/50\n",
      "2588/2588 [==============================] - 0s 114us/sample - loss: 8.8477e-04 - mean_squared_error: 8.8477e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 43/50\n",
      "2588/2588 [==============================] - 0s 114us/sample - loss: 8.8280e-04 - mean_squared_error: 8.8280e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 44/50\n",
      "2588/2588 [==============================] - 0s 127us/sample - loss: 8.5175e-04 - mean_squared_error: 8.5175e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 45/50\n",
      "2588/2588 [==============================] - 0s 122us/sample - loss: 7.8282e-04 - mean_squared_error: 7.8282e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 46/50\n",
      "2588/2588 [==============================] - 0s 112us/sample - loss: 7.8397e-04 - mean_squared_error: 7.8397e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 47/50\n",
      "2588/2588 [==============================] - 0s 111us/sample - loss: 8.7159e-04 - mean_squared_error: 8.7159e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "2588/2588 [==============================] - 0s 94us/sample - loss: 8.0338e-04 - mean_squared_error: 8.0338e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 49/50\n",
      "2588/2588 [==============================] - 0s 89us/sample - loss: 8.6574e-04 - mean_squared_error: 8.6574e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 50/50\n",
      "2588/2588 [==============================] - 0s 97us/sample - loss: 7.8349e-04 - mean_squared_error: 7.8349e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0014 - mean_squared_error: 0.00 - 0s 39us/sample - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "MSE: 0.001844163634814322\n",
      "Mean squared error: 0.00172722\n",
      "Coefficient of determination: 0.89150260\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=31, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "#print(model.summary())\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "print(\"Start training\")\n",
    "# train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)\n",
    "\n",
    "# evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)[1]\n",
    "print(f'MSE: {mse}')\n",
    "y_pred_comb=model.predict(X_test)\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.8f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortened data\n",
    "X_train = npl_En_con_short[:-1000] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_short[-1000:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-1000]\n",
    "y_train = npEn_con[:-1000].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-1000:]\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Train on 2588 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "2588/2588 [==============================] - 1s 247us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 2/50\n",
      "2588/2588 [==============================] - 0s 129us/sample - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 3/50\n",
      "2588/2588 [==============================] - 0s 60us/sample - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 4/50\n",
      "2588/2588 [==============================] - 0s 63us/sample - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 5/50\n",
      "2588/2588 [==============================] - 0s 70us/sample - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 6/50\n",
      "2588/2588 [==============================] - 0s 49us/sample - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 7/50\n",
      "2588/2588 [==============================] - 0s 59us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 8/50\n",
      "2588/2588 [==============================] - 0s 49us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 9/50\n",
      "2588/2588 [==============================] - 0s 59us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 10/50\n",
      "2588/2588 [==============================] - 0s 61us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 11/50\n",
      "2588/2588 [==============================] - 0s 57us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 12/50\n",
      "2588/2588 [==============================] - 0s 56us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 13/50\n",
      "2588/2588 [==============================] - 0s 68us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 14/50\n",
      "2588/2588 [==============================] - 0s 54us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 15/50\n",
      "2588/2588 [==============================] - 0s 56us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 16/50\n",
      "2588/2588 [==============================] - 0s 55us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 17/50\n",
      "2588/2588 [==============================] - 0s 61us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 18/50\n",
      "2588/2588 [==============================] - 0s 48us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 19/50\n",
      "2588/2588 [==============================] - 0s 56us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 20/50\n",
      "2588/2588 [==============================] - 0s 49us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 21/50\n",
      "2588/2588 [==============================] - 0s 68us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 22/50\n",
      "2588/2588 [==============================] - 0s 49us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 23/50\n",
      "2588/2588 [==============================] - 0s 55us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 24/50\n",
      "2588/2588 [==============================] - 0s 52us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 25/50\n",
      "2588/2588 [==============================] - 0s 53us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 26/50\n",
      "2588/2588 [==============================] - 0s 65us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 27/50\n",
      "2588/2588 [==============================] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011       - 0s 46us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 28/50\n",
      "2588/2588 [==============================] - 0s 58us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 29/50\n",
      "2588/2588 [==============================] - 0s 57us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 30/50\n",
      "2588/2588 [==============================] - 0s 88us/sample - loss: 9.9683e-04 - mean_squared_error: 9.9683e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 31/50\n",
      "2588/2588 [==============================] - 0s 50us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 32/50\n",
      "2588/2588 [==============================] - 0s 61us/sample - loss: 9.9923e-04 - mean_squared_error: 9.9923e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 33/50\n",
      "2588/2588 [==============================] - 0s 73us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 34/50\n",
      "2588/2588 [==============================] - 0s 72us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 35/50\n",
      "2588/2588 [==============================] - 0s 70us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 36/50\n",
      "2588/2588 [==============================] - 0s 48us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 37/50\n",
      "2588/2588 [==============================] - 0s 62us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 38/50\n",
      "2588/2588 [==============================] - 0s 47us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 39/50\n",
      "2588/2588 [==============================] - 0s 48us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 40/50\n",
      "2588/2588 [==============================] - 0s 55us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 41/50\n",
      "2588/2588 [==============================] - 0s 53us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 42/50\n",
      "2588/2588 [==============================] - 0s 58us/sample - loss: 9.8197e-04 - mean_squared_error: 9.8197e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 43/50\n",
      "2588/2588 [==============================] - 0s 47us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 44/50\n",
      "2588/2588 [==============================] - 0s 56us/sample - loss: 9.8374e-04 - mean_squared_error: 9.8374e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 45/50\n",
      "2588/2588 [==============================] - 0s 61us/sample - loss: 9.7003e-04 - mean_squared_error: 9.7003e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 46/50\n",
      "2588/2588 [==============================] - 0s 66us/sample - loss: 9.9944e-04 - mean_squared_error: 9.9944e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 47/50\n",
      "2588/2588 [==============================] - 0s 51us/sample - loss: 9.7462e-04 - mean_squared_error: 9.7462e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 48/50\n",
      "2588/2588 [==============================] - 0s 62us/sample - loss: 9.5816e-04 - mean_squared_error: 9.5816e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "2588/2588 [==============================] - 0s 49us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 50/50\n",
      "2588/2588 [==============================] - 0s 61us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "1000/1000 [==============================] - 0s 24us/sample - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "MSE: 0.001739345956593752\n",
      "Mean squared error: 0.00172722\n",
      "Coefficient of determination: 0.89150260\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, input_dim=28, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "#print(model.summary())\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "print(\"Start training\")\n",
    "# train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)\n",
    "\n",
    "# evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)[1]\n",
    "print(f'MSE: {mse}')\n",
    "y_pred_comb=model.predict(X_test)\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.8f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now allow months with NaN data to see if this can improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_t=1200\n",
    "NaN_t2=700\n",
    "En_con=[]\n",
    "l_En_con_short=[]\n",
    "\n",
    "for i in range(df.shape[0]): # loop over all users\n",
    "    meter=df.iloc[i] # = row number i  \n",
    "    \n",
    "    for m in range (2,13): # loop over all months\n",
    "        #load current month\n",
    "        bi, ei =get_monthi(m)               #bi (begin index) ei(end index)\n",
    "        month=meter[bi:ei]  #data for the month m for the row user (row) i\n",
    "        #Load last month\n",
    "        lbi,lei=get_monthi(m-1)             #lbi (last begin index) lei (last end index)\n",
    "        last_month=meter[lbi:lei] \n",
    "        \n",
    "        # Count the amount of NaN's in current and last month      \n",
    "        n_NaN=month.isnull().sum() \n",
    "        l_n_NaN=last_month.isnull().sum()\n",
    "        \n",
    "        # Check if there are any NaN's in current and last month\n",
    "        if n_NaN<NaN_t and l_n_NaN<NaN_t2:\n",
    "            En_con.append(month.mean())\n",
    "            # number of days in the last month\n",
    "            n_day=np.int((lei-lbi+1)/48)\n",
    "            # daily consumption last month\n",
    "            daily_con=np.zeros(28)\n",
    "            mean_lm=last_month.mean()\n",
    "            for j in range(28):\n",
    "                daily_con[j]=last_month[j*48:(j+1)*48].mean()\n",
    "                #if a day happens to be full of NaN we will impute this by the mean\n",
    "                if np.isnan(daily_con[j]):\n",
    "                    daily_con[j]=mean_lm\n",
    "            l_En_con_short.append(daily_con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17756, 28)\n"
     ]
    }
   ],
   "source": [
    "npl_En_con_short=np.array(l_En_con_short)\n",
    "print(npl_En_con_short.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5326\n"
     ]
    }
   ],
   "source": [
    "splitnum=np.int(0.3*npl_En_con_short.shape[0])\n",
    "print(splitnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-9811b125ca5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Make predictions using the testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 506\u001b[1;33m                                    y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[1;32m--> 805\u001b[1;33m                         ensure_2d=False, dtype=None)\n\u001b[0m\u001b[0;32m    806\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    651\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[1;32m--> 653\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "splitnum=np.int(0.3*npl_En_con_short.shape[0])\n",
    "X_train = npl_En_con_short[:-splitnum] #70% trainning set\n",
    "\n",
    "X_test = npl_En_con_short[-splitnum:] #  last 6322 elements are the test set\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-splitnum]\n",
    "y_train = npEn_con[:-splitnum].reshape(-1,1)\n",
    "\n",
    "y_test = npEn_con[-splitnum:]\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
