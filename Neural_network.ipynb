{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"consumption.csv\")\n",
    "weather_avg = pd.read_csv('weather-avg.csv')\n",
    "weather_min = pd.read_csv('weather-min.csv')\n",
    "weather_max = pd.read_csv('weather-max.csv')\n",
    "info = pd.read_csv('addinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_avg = weather_avg.set_index(\"meter_id\")\n",
    "weather_min = weather_min.set_index(\"meter_id\")\n",
    "weather_max = weather_max.set_index(\"meter_id\")\n",
    "info = info.set_index(\"meter_id\")\n",
    "weather_avg_sorted = weather_avg.reindex(df.iloc[:,0])\n",
    "weather_min_sorted = weather_min.reindex(df.iloc[:,0])\n",
    "weather_max_sorted = weather_max.reindex(df.iloc[:,0])\n",
    "info_sorted = info.reindex(df.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the mean value for the NaN values in num_bedrooms column:\n",
    "brinfo=info_sorted['num_bedrooms']\n",
    "values = {'num_bedrooms' : brinfo.mean()}\n",
    "info_filled_br = info_sorted.fillna(value = values)\n",
    "brinfo_filled = info_filled_br['num_bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthi(n):\n",
    "    begin=48*31*(n-1)+1\n",
    "    end=48*31*n\n",
    "    if n==1:\n",
    "        begin=1\n",
    "    if n>1:\n",
    "        end-=3*48\n",
    "    if n>2:\n",
    "        begin-=3*48\n",
    "    if n>3:\n",
    "        end-=48\n",
    "    if n>4:\n",
    "        begin-=48\n",
    "    if n>5:\n",
    "        end-=48\n",
    "    if n>6:\n",
    "        begin-=48\n",
    "    if n>8:\n",
    "        end-=48\n",
    "    if n>9:\n",
    "        begin-=48\n",
    "    if n>10:\n",
    "        end-=48\n",
    "    if n>11:\n",
    "        begin-=48\n",
    "    return begin,end\n",
    "\n",
    "def get_mean_temp(row,month):\n",
    "    \"\"\"\n",
    "    row: is the row (meter_id) we would like to get the average temperature for.\n",
    "    month: which month (columns) we would get the average temperature for.\n",
    "    returns: the average temperature for a specific meter_id for a specific month.\n",
    "    \"\"\"\n",
    "    if month==1:\n",
    "        return row.loc[:,\"2017-01-01 00:00:00\":\"2017-01-31 00:00:00\"].mean(1)\n",
    "    elif month==2:\n",
    "        return row.loc[:,\"2017-02-01 00:00:00\":\"2017-02-28 00:00:00\"].mean(1)\n",
    "    elif month==3:\n",
    "        return row.loc[:,\"2017-03-01 00:00:00\":\"2017-03-31 00:00:00\"].mean(1)\n",
    "    elif month==4:\n",
    "        return row.loc[:,\"2017-04-01 00:00:00\":\"2017-04-30 00:00:00\"].mean(1)\n",
    "    elif month==5:\n",
    "        return row.loc[:,\"2017-05-01 00:00:00\":\"2017-05-31 00:00:00\"].mean(1)\n",
    "    elif month==6:\n",
    "        return row.loc[:,\"2017-06-01 00:00:00\":\"2017-06-30 00:00:00\"].mean(1)\n",
    "    elif month==7:\n",
    "        return row.loc[:,\"2017-07-01 00:00:00\":\"2017-07-31 00:00:00\"].mean(1)\n",
    "    elif month==8:\n",
    "        return row.loc[:,\"2017-08-01 00:00:00\":\"2017-08-31 00:00:00\"].mean(1)\n",
    "    elif month==9:\n",
    "        return row.loc[:,\"2017-09-01 00:00:00\":\"2017-09-30 00:00:00\"].mean(1)\n",
    "    elif month==10:\n",
    "        return row.loc[:,\"2017-10-01 00:00:00\":\"2017-10-31 00:00:00\"].mean(1)\n",
    "    elif month==11:\n",
    "        return row.loc[:,\"2017-11-01 00:00:00\":\"2017-11-30 00:00:00\"].mean(1)\n",
    "    elif month==12:\n",
    "        return row.loc[:,\"2017-12-01 00:00:00\":\"2017-12-31 00:00:00\"].mean(1)\n",
    "    else:\n",
    "        print(\"Error: this is not a valid input for month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps=[]\n",
    "temps_min=[]\n",
    "temps_max=[]\n",
    "En_con=[]\n",
    "month_arr=[]\n",
    "br_arr=[]\n",
    "l_En_con=[]\n",
    "NaN_t=1200\n",
    "\n",
    "for i in range(df.shape[0]): # loop over all users\n",
    "    \n",
    "    meter=df.iloc[i] # = row number i  \n",
    "    \n",
    "    fmf=False;        # first month found\n",
    "    \n",
    "    \n",
    "    for m in range (1,13): # loop over all months\n",
    "        begin_index, end_index =get_monthi(m)               #get index of beginning and end of month\n",
    "        month=meter[begin_index:end_index] #data for the month m for the row user (row) i\n",
    "        row = brinfo_filled[i:i+1]\n",
    "\n",
    "        # Check if months have numeric values otherwise discard the month for this user.       \n",
    "        n_NaN=month.isnull().sum() #Number of NaN's\n",
    "        if n_NaN<NaN_t:\n",
    "            if fmf:\n",
    "                #current month\n",
    "                temps.append(get_mean_temp(weather_avg_sorted.iloc[i:i+1],m))\n",
    "                temps_min.append(get_mean_temp(weather_min_sorted.iloc[i:i+1],m))\n",
    "                temps_max.append(get_mean_temp(weather_max_sorted.iloc[i:i+1],m))\n",
    "                En_con.append(month.mean())\n",
    "                month_arr.append(m) # = number of data points = the months that passed threshold for each user.\n",
    "                br_arr.append(row[0])\n",
    "                #last month\n",
    "                l_begin_index,l_end_index=get_monthi(m-1)\n",
    "                last_month=meter[l_begin_index:l_end_index] #load last month\n",
    "                l_En_con.append(last_month.mean())\n",
    "            else:\n",
    "                fmf=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list to numpy arrays: \n",
    "nptemps = np.zeros(len(temps))\n",
    "nptemps_min = np.zeros(len(temps_min))\n",
    "nptemps_max = np.zeros(len(temps_max))\n",
    "npEn_con = np.zeros(len(En_con))\n",
    "npmonth_arr = np.zeros(len(month_arr))\n",
    "npl_En_con= np.zeros(len(l_En_con))\n",
    "npbr_arr = np.zeros(len(br_arr)) #converting it into numpy array.\n",
    "\n",
    "for i in range(len(temps)):\n",
    "     nptemps[i] = temps[i]\n",
    "\n",
    "for i in range(len(temps_min)):\n",
    "     nptemps_min[i] = temps_min[i]\n",
    "\n",
    "for i in range(len(temps_max)):\n",
    "     nptemps_max[i] = temps_max[i]\n",
    "\n",
    "for i in range(len(En_con)):\n",
    "     npEn_con[i] = En_con[i]\n",
    "\n",
    "for i in range(len(month_arr)):\n",
    "     npmonth_arr[i] = month_arr[i]\n",
    "        \n",
    "for i in range(len(En_con)):\n",
    "     npl_En_con[i] = l_En_con[i]\n",
    "\n",
    "for i in range(len(br_arr)):\n",
    "    npbr_arr[i] = br_arr[i]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_features= np.zeros(shape = (len(temps), 6))\n",
    "\n",
    "for i in range(len(temps)):\n",
    "    np_features[i, 0]  = l_En_con[i]\n",
    "    np_features[i, 1] = temps[i]\n",
    "    np_features[i, 2] = temps_min[i]\n",
    "    np_features[i, 3] = temps_max[i]\n",
    "    np_features[i, 4] = npbr_arr[i]\n",
    "    np_features[i, 5] = npmonth_arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitnum=5347\n",
    "# Split the data into training/testing sets\n",
    "X_train_comb = np_features[:-splitnum]\n",
    "X_test_comb = np_features[-splitnum:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "y_train = npEn_con[:-splitnum]\n",
    "y_test = npEn_con[-splitnum:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 256)               1792      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2566)              659462    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 2567      \n",
      "=================================================================\n",
      "Total params: 729,613\n",
      "Trainable params: 729,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=6, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Train on 12479 samples, validate on 5347 samples\n",
      "Epoch 1/50\n",
      "12479/12479 [==============================] - 4s 339us/sample - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 2/50\n",
      "12479/12479 [==============================] - 4s 315us/sample - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 3/50\n",
      "12479/12479 [==============================] - 4s 317us/sample - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 4/50\n",
      "12479/12479 [==============================] - 4s 338us/sample - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 5/50\n",
      "12479/12479 [==============================] - 4s 319us/sample - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 6/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 7/50\n",
      "12479/12479 [==============================] - 4s 343us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 8/50\n",
      "12479/12479 [==============================] - 4s 324us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 9/50\n",
      "12479/12479 [==============================] - 4s 325us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 10/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 11/50\n",
      "12479/12479 [==============================] - 4s 339us/sample - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 12/50\n",
      "12479/12479 [==============================] - 4s 318us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 13/50\n",
      "12479/12479 [==============================] - 4s 317us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 14/50\n",
      "12479/12479 [==============================] - 4s 323us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 15/50\n",
      "12479/12479 [==============================] - 4s 336us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 16/50\n",
      "12479/12479 [==============================] - 4s 321us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 17/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 18/50\n",
      "12479/12479 [==============================] - 4s 321us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 19/50\n",
      "12479/12479 [==============================] - 4s 333us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 20/50\n",
      "12479/12479 [==============================] - 4s 327us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 21/50\n",
      "12479/12479 [==============================] - 4s 338us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 22/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 23/50\n",
      "12479/12479 [==============================] - 4s 346us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 24/50\n",
      "12479/12479 [==============================] - 4s 327us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 25/50\n",
      "12479/12479 [==============================] - 4s 333us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 26/50\n",
      "12479/12479 [==============================] - 4s 326us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 27/50\n",
      "12479/12479 [==============================] - 4s 337us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 28/50\n",
      "12479/12479 [==============================] - 4s 324us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 29/50\n",
      "12479/12479 [==============================] - 4s 324us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 30/50\n",
      "12479/12479 [==============================] - 4s 329us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 31/50\n",
      "12479/12479 [==============================] - 4s 335us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 32/50\n",
      "12479/12479 [==============================] - 4s 324us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 33/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 34/50\n",
      "12479/12479 [==============================] - 4s 330us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 35/50\n",
      "12479/12479 [==============================] - 4s 333us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 36/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 37/50\n",
      "12479/12479 [==============================] - 4s 321us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 38/50\n",
      "12479/12479 [==============================] - 4s 327us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 39/50\n",
      "12479/12479 [==============================] - 4s 331us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 40/50\n",
      "12479/12479 [==============================] - 4s 326us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 41/50\n",
      "12479/12479 [==============================] - 4s 321us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 42/50\n",
      "12479/12479 [==============================] - 4s 334us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 43/50\n",
      "12479/12479 [==============================] - 4s 329us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 44/50\n",
      "12479/12479 [==============================] - 4s 322us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 45/50\n",
      "12479/12479 [==============================] - 4s 326us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 46/50\n",
      "12479/12479 [==============================] - 4s 338us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 47/50\n",
      "12479/12479 [==============================] - 4s 326us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 48/50\n",
      "12479/12479 [==============================] - 4s 316us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 49/50\n",
      "12479/12479 [==============================] - 4s 315us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 50/50\n",
      "12479/12479 [==============================] - 4s 324us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "5347/5347 [==============================] - 0s 82us/sample - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "MSE: 0.002361157676205039\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "print(\"Start training\")\n",
    "# train the model\n",
    "model.fit(X_train_comb, y_train, validation_data=(X_test_comb, y_test), epochs=50)\n",
    "\n",
    "# evaluate the model\n",
    "mse = model.evaluate(X_test_comb, y_test)[1]\n",
    "print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.00236116\n",
      "Coefficient of determination: 0.8876\n"
     ]
    }
   ],
   "source": [
    "y_pred_comb=model.predict(X_test_comb)\n",
    "print('Mean squared error: %.8f'\n",
    "      % mean_squared_error(y_test, y_pred_comb))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.4f'\n",
    "      % r2_score(y_test, y_pred_comb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch=10 <br>\n",
    "Mean squared error: 0.00253821 <br>\n",
    "Coefficient of determination: 0.8792 <br>\n",
    "This is a slight improvement on the polynomial model,although there is probably some variation on this <br>\n",
    "epoch=50 <br>\n",
    "Mean squared error: 0.00236116 <br>\n",
    "Coefficient of determination: 0.8876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: Januari (or other month) specific test set on a model. This can be used to see if model actually know how to deal with these months if which there is very little training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
